/**
 * 第3章クイズデータ - ニューラルネットワーク要素技術
 */
export const chapter3Quiz = [
    {
        id: 'ch3_q1',
        question: 'パーセプトロンの限界として知られているものは？',
        options: [
            '計算速度が遅い',
            '線形分離不可能な問題（XOR等）を解けない',
            'メモリ使用量が多い',
            '画像処理ができない'
        ],
        correctIndex: 1,
        explanation: '単純パーセプトロンは線形分離可能な問題しか解けません。XOR問題のような非線形問題には対応できません。',
        keywords: ['パーセプトロン', 'XOR問題', '線形分離'],
        hint: '直線1本で分類できない問題に対する限界があります。'
    },
    {
        id: 'ch3_q2',
        question: '活性化関数の役割として正しいものは？',
        options: [
            '計算を高速化する',
            '非線形性を導入する',
            'データを圧縮する',
            'メモリを節約する'
        ],
        correctIndex: 1,
        explanation: '活性化関数は非線形変換を行い、ニューラルネットワークが複雑なパターンを学習できるようにします。',
        keywords: ['活性化関数', '非線形性'],
        hint: 'これがないと、何層重ねても線形変換の組み合わせにしかなりません。'
    },
    {
        id: 'ch3_q3',
        question: 'ReLU関数の数式として正しいものは？',
        options: [
            'f(x) = 1 / (1 + e^(-x))',
            'f(x) = max(0, x)',
            'f(x) = tanh(x)',
            'f(x) = x^2'
        ],
        correctIndex: 1,
        explanation: 'ReLU（Rectified Linear Unit）は負の値を0に、正の値はそのまま出力します。',
        keywords: ['ReLU', '活性化関数'],
        hint: '「負ならゼロ、正ならそのまま」というシンプルな関数です。'
    },
    {
        id: 'ch3_q4',
        question: 'シグモイド関数の出力範囲は？',
        options: [
            '-∞ から +∞',
            '-1 から 1',
            '0 から 1',
            '0 から +∞'
        ],
        correctIndex: 2,
        explanation: 'シグモイド関数は入力を0から1の範囲に圧縮します。確率の出力などに使われます。',
        keywords: ['シグモイド', '出力範囲'],
        hint: '確率として解釈できる範囲に出力を収めます。'
    },
    {
        id: 'ch3_q5',
        question: '多クラス分類の出力層でよく使われる活性化関数は？',
        options: [
            'ReLU',
            'シグモイド',
            'tanh',
            'ソフトマックス'
        ],
        correctIndex: 3,
        explanation: 'ソフトマックス関数は各クラスの確率を出力し、合計が1になります。多クラス分類の出力に適しています。',
        keywords: ['ソフトマックス', '多クラス分類'],
        hint: '複数の選択肢それぞれの確率を求める時に使います。'
    },
    {
        id: 'ch3_q6',
        question: '回帰問題でよく使われる誤差関数は？',
        options: [
            '交差エントロピー',
            '平均二乗誤差（MSE）',
            'ヒンジ損失',
            'KLダイバージェンス'
        ],
        correctIndex: 1,
        explanation: '平均二乗誤差は予測値と実際の値の差の二乗の平均で、回帰問題で広く使われます。',
        keywords: ['MSE', '平均二乗誤差', '回帰'],
        hint: '予測と正解の「距離」を測る最も基本的な方法の一つです。'
    },
    {
        id: 'ch3_q7',
        question: '分類問題でよく使われる誤差関数は？',
        options: [
            '平均二乗誤差',
            '平均絶対誤差',
            '交差エントロピー',
            'コサイン類似度'
        ],
        correctIndex: 2,
        explanation: '交差エントロピーは確率分布間の「距離」を測り、分類問題で広く使われます。',
        keywords: ['交差エントロピー', '分類', '確率分布'],
        hint: '確率として出力された予測と、正解の差を測る関数です。'
    },
    {
        id: 'ch3_q8',
        question: '誤差逆伝播法で使用される数学的な規則は？',
        options: [
            'ピタゴラスの定理',
            '連鎖律（チェーンルール）',
            'ベイズの定理',
            '大数の法則'
        ],
        correctIndex: 1,
        explanation: '誤差逆伝播法は、連鎖律を使って各層の勾配を効率的に計算します。',
        keywords: ['誤差逆伝播法', '連鎖律', '勾配'],
        hint: '合成関数の微分を計算するためのルールです。'
    },
    {
        id: 'ch3_q9',
        question: '勾配消失問題が起きやすい活性化関数は？',
        options: [
            'ReLU',
            'Leaky ReLU',
            'シグモイド',
            'GELU'
        ],
        correctIndex: 2,
        explanation: 'シグモイド関数は入力が大きい/小さい領域で勾配がほぼ0になり、勾配消失問題を引き起こしやすいです。',
        keywords: ['勾配消失', 'シグモイド'],
        hint: '出力が0か1に近づくと、微分値がほぼ0になってしまう関数です。'
    },
    {
        id: 'ch3_q10',
        question: '確率的勾配降下法（SGD）の特徴として正しいものは？',
        options: [
            '全データを使って1回で更新',
            'ランダムに選んだ一部のデータで更新',
            '勾配を使わずに更新',
            '学習率を固定して使用'
        ],
        correctIndex: 1,
        explanation: 'SGDはランダムに選んだミニバッチ（一部のデータ）で勾配を計算し、パラメータを更新します。',
        keywords: ['SGD', '確率的勾配降下法', 'ミニバッチ'],
        hint: '毎回全データを使わず、一部だけでパラメータを更新していきます。'
    },
    {
        id: 'ch3_q11',
        question: 'Adamオプティマイザの特徴は？',
        options: [
            '学習率が固定',
            'モーメンタムとRMSpropを組み合わせたもの',
            '二次微分を使用',
            '遺伝的アルゴリズムを使用'
        ],
        correctIndex: 1,
        explanation: 'Adamはモーメンタム（過去の勾配の慣性）とRMSprop（学習率の適応的調整）を組み合わせた最適化手法です。',
        keywords: ['Adam', 'モーメンタム', 'RMSprop'],
        hint: '2つの人気手法の長所を組み合わせています。'
    },
    {
        id: 'ch3_q12',
        question: 'L2正則化の効果として正しいものは？',
        options: [
            '重みをゼロにする',
            '重みを小さく保つ',
            '学習を高速化する',
            'データを増やす'
        ],
        correctIndex: 1,
        explanation: 'L2正則化は重みの二乗和にペナルティを課し、重みを小さく保って過学習を防ぎます。',
        keywords: ['L2正則化', 'Ridge', '重み減衰'],
        hint: '大きすぎる重みにペナルティを与える方法です。'
    },
    {
        id: 'ch3_q13',
        question: 'ドロップアウトの仕組みとして正しいものは？',
        options: [
            'データをランダムに削除する',
            '学習時にランダムにニューロンを無効化する',
            '層の数を減らす',
            '学習率を下げる'
        ],
        correctIndex: 1,
        explanation: 'ドロップアウトは学習時にランダムにニューロンを無効化し、過学習を防ぎます。推論時は全ニューロンを使用します。',
        keywords: ['ドロップアウト', '正則化', '過学習'],
        hint: '「サボる」ニューロンをランダムに作ることで、モデルの汎化性能を高めます。'
    },
    {
        id: 'ch3_q14',
        question: 'バッチサイズを大きくした場合の影響は？',
        options: [
            '勾配の推定が安定する',
            'メモリ使用量が減少する',
            '学習が不安定になる',
            '過学習しにくくなる'
        ],
        correctIndex: 0,
        explanation: 'バッチサイズが大きいと、多くのサンプルの平均で勾配を計算するため、推定が安定します。',
        keywords: ['バッチサイズ', '勾配', '安定性'],
        hint: 'より多くのデータで平均を取ると、ブレが少なくなります。'
    },
    {
        id: 'ch3_q15',
        question: '学習率が大きすぎると何が起こるか？',
        options: [
            '学習が遅くなる',
            '損失が発散する',
            '過学習する',
            '勾配が消失する'
        ],
        correctIndex: 1,
        explanation: '学習率が大きすぎると、最適解を飛び越えてしまい、損失が発散（無限大に向かう）することがあります。',
        keywords: ['学習率', '発散'],
        hint: '坂道を駆け下りる時、歩幅が大きすぎると転げ落ちるイメージです。'
    },
    {
        id: 'ch3_q16',
        question: '早期終了（Early Stopping）の目的は？',
        options: [
            '学習を高速化する',
            '過学習を防ぐ',
            'メモリを節約する',
            'データを減らす'
        ],
        correctIndex: 1,
        explanation: '早期終了は検証誤差が悪化し始めたら学習を止めることで、過学習を防ぎます。',
        keywords: ['早期終了', 'Early Stopping', '過学習'],
        hint: '「これ以上続けると逆効果」というタイミングで止める手法です。'
    },
    {
        id: 'ch3_q17',
        question: 'エポック（Epoch）とは？',
        options: [
            '1回のパラメータ更新',
            '全訓練データを1回学習すること',
            '検証データでの評価',
            'テストデータでの評価'
        ],
        correctIndex: 1,
        explanation: '1エポックは、全訓練データを1回通して学習することを指します。',
        keywords: ['エポック', '訓練データ'],
        hint: '全データを「一周」することを表す単位です。'
    },
    {
        id: 'ch3_q18',
        question: '重みの初期化が不適切だと何が起こりうるか？',
        options: [
            'メモリ不足になる',
            '勾配消失や勾配爆発が起こりやすくなる',
            'CPUの使用率が上がる',
            'データが破損する'
        ],
        correctIndex: 1,
        explanation: '不適切な初期化は、学習初期から勾配が極端に小さくなったり大きくなったりする原因になります。',
        keywords: ['重み初期化', '勾配消失', '勾配爆発'],
        hint: 'スタート時点での設定が、その後の学習全体に影響を与えます。'
    }
];
